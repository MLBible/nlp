<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Transformers</title>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        processEscapes: true
      },
      "HTML-CSS": {
        linebreaks: {
          automatic: true
        },
        availableFonts: ["TeX"]
      },
      showMathMenu: true
    });
  </script>
</head>
<body>
  <h1>Transformers</h1>
  <ul><li><a href="#introduction-to-attention">Introduction to Attention</a></li><li><a href="#computing-the-attention-weights">Computing the Attention Weights</a><ul><li><a href="#dot-product-multiplicative">Dot-Product (Multiplicative)</a></li><li><a href="#scaled-dot-product-attention">Scaled Dot-Product Attention</a></li></ul></li><li><a href="#self-attention-queries-keys-and-values">Self-Attention: Queries, Keys and Values</a></li><li><a href="#multi-headed-attention">Multi-Headed Attention</a></li><li><a href="#positional-encoding">Positional Encoding</a></li></ul>
  <h1 id="introduction-to-attention">Introduction to Attention</h1>

<p>Consider the following two sentences:</p>

<center>1. <i>The dog did not cross the road as <u>it</u> was too tired.</i></center>
<center>2. <i>The dog did not cross the road as <u>it</u> was too narrow.</i></center>

<p>In the first sentence, the pronoun <em>it</em> refers to dog whereas in the second sentence <em>it</em> refers to the road. We need our model to understand this relationship. To achieve this, we can use the whole sequence to compute a <em>weighted average</em> of each embedding instead of using a fixed embedding for each token. Another way to formulate this is to say that given a sequence of token embeddings \(x_1, \ldots, x_n\),  we produce a sequence of new embeddings \(x_1', \ldots, x_n'\) where each \(x_i'\) is a linear combination of all the \(x_j\):</p>

\[x_i' = \sum_{j = 1}^{n} w_{ji}x_j\]

<p>The coefficients \(w_{ji}\) are called <em>attention weights</em> and are normalized so that \(\sum_j w_{ji} = 1\). The weighted averaging scheme would probably assign a higher weight to the word <em>dog</em> when creating the embedding for the word <em>it</em> for our first sentence example and would assign a higher weight on <em>road</em> for the second one. Embeddings that are generated in this way are called <em>contextualized embeddings</em> and predate the invention of transformers in language models like ELMo.</p>

<blockquote style="background-color: #FFFFE0; padding: 10px;">
<b>
Note: We use the terms <i>token</i> and <i>word</i> interchangeably here, even though they could be different. More on this later.
</b>
</blockquote>

<h1 id="computing-the-attention-weights">Computing the Attention Weights</h1>

<h2 id="dot-product-multiplicative">Dot-Product (Multiplicative)</h2>

<p>The dot product-based scoring function is the simplest one and has no parameters to tune. We can compute the dot product between \(x_i\) and \(x_j\) to compute \(w_{ij}\). Dot product indicates how similar \(x_i\) is to \(x_j\) and can hence be used as a weight.</p>

\[w_{ji} = x_i \cdot x_j\]

<h2 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h2>

<p>The scaled dot product-based scoring function divides the dot product by the square root of the dimension. According to Vaswani et al., as the dimension increases, the dot products grow larger, which pushes the softmax function into regions with extreme gradients.</p>

\[w_{ji} = \frac{x_i \cdot x_j}{\sqrt{d}}\]

<p>To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$. Then their dot product, \(x_i \cdot x_j = \sum_{r=1}^{d} x_{ir} x_{jr}\), has mean $0$ and variance \(d\).</p>

<p>In our simple example, we only used the embeddings “as is” to compute the attention scores and weights. Though this mechanism is simple, there is no learning of weights happening in this. And this is where the mechanism of query and key matrices comes into the picture.  In practice, the self-attention layer applies three independent linear transformations to each embedding to generate the query, key, and value vectors. These transformations project the embeddings and each projection carries its own set of learnable parameters, which allows the self-attention layer to focus on different semantic aspects of the sequence.</p>

<h1 id="self-attention-queries-keys-and-values">Self-Attention: Queries, Keys and Values</h1>

<p>The concept of self-attention is based on three vector representations:</p>

<ol>
  <li>Query</li>
  <li>Key</li>
  <li>Value</li>
</ol>

<p>Instead of directly finding the dot product between \(x_i\) and \(x_j\), we project each token embedding into three vectors called <em>query</em>, <em>key</em>, and <em>value</em>.</p>

<p>Consider sentence 1 again for example. Let’s focus on finding the embedding for the token <em>it</em>. Assume that the dimension of all the tokens is 512 i.e. we represent each token through a dense vector of length 512. All these embeddings are randomly initialised before training and are learned during training the model.</p>

<p>Instead of finding the dot product between <em>it</em> and all other tokens in the sentence and using them as the weights, we do the following:</p>

<ul>
  <li>
    <p>Step 1: Define a query matrix \(Q\) of dimension \(512 \times 64\) and multiply the embedding vector of <em>it</em> with this matrix to project the embedding of <em>it</em> into a \(64\)-dimensional space. Let us call this new vector as <em>query</em>.</p>
  </li>
  <li>
    <p>Step 2: Define a key matrix \(K\) of dimension \(512 \times 64\) and multiply all the token embeddings (including the token <em>it</em>) with this matrix. All the tokens are now projected into another \(64\)-dimensional space. Let us call these vectors as <em>keys</em>.</p>
  </li>
  <li>
    <p>Step 3: Define a value matrix \(V\) of dimension \(512 \times 64\) and multiply all the token embeddings (including the token <em>it</em>) with this matrix. All the tokens are now projected into another \(64\)-dimensional space. Let us call these vectors $v_1, \ldots, v_n$ as <em>values</em>.</p>
  </li>
  <li>
    <p>Step 4: Determine how much the query and key vectors relate to each other using a similarity function (scaled dot-product attention). Our attention mechanism with equal query and key vectors will assign a very large score to identical words in the context, and in particular to the current word itself (and hence the name <em>self-attention</em>). The outputs from this step are called the attention scores, and for a sequence with \(n\) input tokens there is a corresponding \(n \times n\) matrix of attention scores.</p>
  </li>
  <li>
    <p>Step 5: Multiply the attention scores by a scaling factor to normalize their variance and then normalized with a softmax to ensure all the column values sum to 1. This is done in order to make it possible to build gradients that are more stable. The resulting \(n \times n\) matrix now contains all the attention weights, \(w_{ji}\).</p>
  </li>
  <li>
    <p>Step 6: Update the token embeddings. Once the attention weights are computed, we multiply them by the value vector $v_1, \ldots, v_n$ to obtain an updated representation for embedding of the token <em>it</em> $x_i’$:</p>
  </li>
</ul>

\[x_i' = \sum_j w_{ji} v_j\]

<p>Similarly, we can find the embeddings of all other tokens by casting them into their query vectors using the same query matrix \(Q\), finding their attention weights using the key vectors and using these weights to compute the weighted average of the value vectors.</p>

<p>Instead of a vector computation for each token $i$, the input matrix $X \in \mathbb{R}^{l \times d}$, where $l$ is the maximum length of the sentence and $d$ is the dimension of the inputs, combines with each of the query, key, and value matrices as a single computation given by:</p>

\[\text{attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\]

<p>where \(d_k=64\) in our case. \(Q, K\) and \(V\) weight matrices are randomly initialized and the weights are jointly learned from the training process.</p>

<p>This self-attention layer helps the model capture the context of the word in its representation for example, it may be about learning grammar, tense, conjugation, etc..</p>

<h1 id="multi-headed-attention">Multi-Headed Attention</h1>

<p>Instead of a single self-attention head, there can be \(h\) parallel self-attention heads; this is known as multi-head attention. Multi-head attention involves multiple sets of query/key/value weight matrices, each resulting in different query/key/value matrices for the inputs. These output matrices from each head are concatenated and multiplied with an additional weight matrix, $W_O$.</p>

<p>In the original transformer paper, the authors used \(h = 8\) heads. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p>

\[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W_O\]

<p>where</p>

\[\text{head}_i = \text{Attention}(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V})\]

<p>The projections are parameter matrices:</p>

<p>\(W_{i}^{Q} \in \mathbb{R}^{d_{\text{model}} \times d_k}, \quad W_{i}^{K} \in \mathbb{R}^{d_{\text{model}} \times d_k}, \quad W_{i}^{V} \in \mathbb{R}^{d_{\text{model}} \times d_v}\) and \(W_O \in \mathbb{R}^{hd_v \times d_{\text{model}}}\)</p>

<p>For each self-attention head, the authors used \(d_k = d_v = \frac{d_{\text{model}}}{h} = 64\). Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.</p>

<p>Each head can learn something different, for example, it may be about learning grammar, tense, conjugation, etc. For instance, one head can focus on subject-verb interaction, whereas another finds nearby adjectives. It also helps the model expand the focus to different positions. Obviously we don’t handcraft these relations into the model, and they are fully learned from the data. If you are familiar with computer vision models you might see the resemblance to filters in convolutional neural networks, where one filter can be responsible for detecting faces and another one finds wheels of cars in images.</p>

<h1 id="positional-encoding">Positional Encoding</h1>

<p>Word order and positions play a crucial role in most of the NLP tasks. By taking one word at a time, recurrent neural networks essentially incorporate word order. In transformer architecture, to gain speed and parallelism, recurrent neural networks are replaced by multi-head attention layers. The multi-head attention layer is effectively a weighted sum and does not take into account token position. Thus it becomes necessary to explicitly pass the information about the word order to the model layer as one way of capturing it. There are several ways to achieve this. We can use a learnable pattern, especially when the pretraining dataset is sufficiently large. This works exactly the same way as the token embeddings, but using the position index instead of the token ID as input. The final output embedding is simply the sum of the input embeddings and positional embedding. While learnable position embeddings are easy to implement and widely used, there are some alternatives.</p>

<ol>
<li>Absolute positional representations: Transformer models can use static patterns consisting of modulated sine and cosine signals to encode the positions of the tokens. This works especially well when there are not large volumes of data available. If the length of the sentence is given by $l$ and the embedding dimension/depth is given by $d$, positional encoding $P$ is a 2-D matrix of the same dimension, i.e., $P \in \mathbb{R}^{l \times d}$. Every position can be represented with equations in terms of $i$ (along $l$) and $j$ (along $d$) dimensions as:

$$
\begin{align}
P_{i,2j} &amp;= \sin\left(\frac{i}{10000^{2j/d}}\right) \\
P_{i,2j+1} &amp;= \cos\left(\frac{i}{10000^{2j/d}}\right) 
\end{align}
$$

for $i = 0, \ldots, l-1$ and $j = 0, \ldots, \left\lfloor \frac{d-1}{2} \right\rfloor$. We use the sin function when $j$ is even and the cos function when $j$ is odd. The function definition above indicates that the frequencies are decreasing along the vector dimension and form a geometric progression from $2\pi$ to $10000 \cdot 2\pi$ on the wavelengths. 
The two matrices, i.e., the word embeddings $W$ and the positional encoding $P$, are added to generate the input representation $X = W + P \in \mathbb{R}^{l \times d}$.
</li>
<br />
<li>Relative positional representations: Relative positional representations encode the relative positions between tokens. Models such as DeBERTa use such representations.
</li>
</ol>


</body>
</html>
